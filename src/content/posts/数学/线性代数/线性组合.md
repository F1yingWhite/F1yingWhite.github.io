---
title: 线性代数
description: ""
image: ""
published: 2025-07-30
tags:
  - 数学
category: 数学
draft: false
---

# 运算封闭

对于一个集合，如果任意两数在一起运算后结果仍在这个集合，那么这个集合对于这种运算是封闭的。当我定义了一种运算后，所可能产生的所有结果是什么？对于向量而言，问题就是，当我初始拥有了一定数量的向量后，对他（们）进行加法和数乘运算，所可能产生的整个向量集合是什么？这就引出了向量空间的概念

# 线性空间

一组基座标定义了一个坐标系，使用不同的基向量就构造了不同的坐标系，将向量缩放在想加就是线性组合。

俩向量 v,w 所有的线性组合就构成了 v,w 张成的空间。

如果我们定义几个向量是线性相关的，意思就是向量组中有至少一个向量可以用其他的向量的线性表示来组合，也就是他对这个向量组没有贡献，把他从向量组中拿掉不影响组成的空间。相反线性无关就是每一个向量都贡献了一个维度，都缺一不可。

# 基

如果向量空间中的一组向量满足：互相线性无关，张成 V，那么他们就是向量空间 V 的一组基。改空间的任意向量都能表达为基向量的线性组合，基向量的向量数叫做维数，记住 dim(V)，同一个向量空间可以有多组基，他们必定维度相等。

# 矩阵与线性变换

变换本质就是函数，在线性代数中我们输入一个向量，输出另一个向量。

## 线性变换

如果变换前后所有的直线仍是直线且远点保持不变，那么就是线性变换。放在二维直角坐标系这一特定场景下，具体的体现就是施加线性变换后，整个坐标系的原点不变，并使网络线保持平行且等距分布。线性变换后的 v 仍然是变换后的 i 和 j 的线性组合。

![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20250730132830705.png)

实际上我们只要知道基向量的位置是怎么变的，其他的向量线性变换后的位置可以快速得到，

简而言之，选定基之后， **向量刻画对象，矩阵刻画对象的运动，用矩阵与向量的乘法施加运动；矩阵的本质是运动的描述。**

## 矩阵乘法

矩阵表示了一种线性变换。有些时候我们会进行多次线性变换，比如对向量先旋转再剪切，但无论经过多少次，最后的总体作用还是一个线性变换，这样的变换可以看做是由多个独立变换组合成的复合变换。

![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20250730133350573.png)

如上图，我们可以看到矩阵，这里矩阵是从右向左不断叠加的，注意这里的复合矩阵就是现旋转再剪切。如此，把矩阵乘法理解为 **连续的几次线性变换** ，我们也能很容易理解，矩阵 A\*B 的结果和矩阵 B\*A 的结果是不一样的，因为操作顺序的不同，产生的影响也不同。比如，先对 i 和 j 基向量先往 x 轴方向拉伸一倍，再顺时针旋转 90 度，与先旋转 90 度再拉伸，结果肯定不一样。

# 行列式

线性变换中有些吧空间向外拉伸，有的向内积压，测量变换具体对空间产生了多少拉伸和压缩对理解变换很有用。

比如我们的基向量变为了 (3,0) 和（0,2），那么 1,1 面积就扩大了 6 倍。实际上，我们只需要观察这个单位正方形变换后的面积变化比例，就等于知道了其他任意区域的面积变化比例，==因为对于其他任意的方块来说都会有相同的变化，这是由线性变换产生网格线保持平行且等距分布这一特性推断出的。而这个变化的比例，就是我们常说的行列式==。如果说一个线性变换的行列式是 3，那就是说它将一个区域的面积变化为原先的 3 倍。

如果一个线性变换的行列式为 0，则说明它将原来的二维平面压缩到了一条线（甚至一个点）上，此时所有区域的面积都为 0。==换句话说，探究一个矩阵的行列式是否为 0，就能了解这个矩阵对应的线性变换是否将空间压缩到了更低维度==（例如从二维降维到一维空间）。

对于负数的行列式就是换了个向

## 行列式的计算

![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20250730135106561.png)

二维行列式的计算简单，上图可以看出空间面积的变化。

det(M1M2)=det(M1)det(M2) 左边的等式代表先进行 M2 矩阵代表的线性变换再执行 M1 所代表的线性变换之后，面积或者体积所变化的比例。右边的式子是两个线性变换使面积或体积变化的比例的乘积。因为两边线性变换之后的结果是一样的（执行顺序一样），所以比例肯定也是一样的

# 逆矩阵、逆矩阵、秩、列空间、与零空间

## 逆矩阵

矩阵的一大作用是解方程，这里的矩阵表示了某个线性变换，求解方程也就等价于寻找一个向量，让他在经过 A 变换后和向量 V 重合

![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20250730140439034.png)

这个方程组 Ax = v 的解依赖于矩阵 A 所代表的变换，是将原始空间挤压到一个更低维空间，还是保持不变，即 A 的行列式是否是 0.如果行列式不为 0，那么空间维度不变，也就是只有一个向量和 v 重合，

对于矩阵求解，我们计算 A 矩阵的逆矩阵就好了

$$
A^{-1}A\vec{x}=A^{-1}\vec{v}
$$

存在逆矩阵的矩阵叫非奇异矩阵，不存在逆矩阵的矩阵叫奇异矩阵。如果行列式为 0，那么 A 代表变换吧空间压缩到了更低的维度，此时 A 没有逆变换，也就是不能吧低维空间转换到高维。如果想要精确的描述变换后的维度，就引入了心的概念，秩

## 秩

**秩** 就代表了变换后空间的维数。如果变换后所有的向量都落在一条直线上，那么这个变换的秩为 1；如果变换后所有的向量都落在一个二维空间上，那么这个变换的秩为 2。
一个矩阵 A 的列秩是 A 的线性无关的列的极大数目。类似地，行秩是 A 的线性无关的行的极大数目。==矩阵的列秩和行秩总是相等的==，因此它们可以简单地称作矩阵 A 的秩。

## 非方阵

之前我们讨论的矩阵都是方阵，例如用 2\*2 的矩阵表示二维向量到二维向量的变换。那么如何理解非方阵呢？很简单，仍然是线性变换，但是是从某个维度转换为另一个维度的坐标。

以一个 3\*2 的矩阵为例，它的几何意义是将输入的二维空间映射到三围空间上。 矩阵有 2 列表示输入空间有 2 个基向量（因此是二维输入空间），有 3 行表示每一个基向量在变换后用 3 个独立的坐标来描述。

# 点积与正交

从几何角度理解，向量的点积就是一个向量在另一个向量上的投影长度，乘另一个向量的长度，因此点积的主要用途是判断两个向量是否正交。当两向量正交的时候，点积的结果为 0。如果方向相反，点积的结果为负值。当方向完全一致时，点积的值最大。那么这二者是如何联系起来的？

首先我们有一个二维空间到数轴的线性变换，因为这个变换是线性的，因此肯定能有一个一行二列的矩阵 (ux,uy) 来表示（从 2 维压缩到 1 维），这个矩阵也就是变换后的基向量 i,j 的坐标，也就是基向量在新数轴上的投影。

![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20250730142920778.png)

而又因为 1\*2 矩阵与一个二维向量相乘的过程，和将这个矩阵转置过来，与向量做点积的过程相同，所以这个投影的变换必定与某个二维向量有关。由此我们得到结论，任何时候一个线性变换，如果输出空间是一维的数轴，不管这个变换是如何定义的，空间都会存在一个唯一的向量与这个变换相关，施加该线性变换和做点积效果是一样的。换句话说，两个向量的点积，就是将其中一个向量转换为线性变换，施加在另一个向量上。

这是数学中对偶性的一个体现。一个向量的对偶，是它定义的线性变换。一个多维空间到一维空间的线性变换的对偶，就是多维空间中的某个特定向量。

## 正定矩阵

正定矩阵在机器学习特别在矩阵分解中有重要的作用，他和向量点积密切相关

定义:A 是 n 阶对称方阵，如果对任何非零向量 x，都有 $x^TAx>0$ ，就称 A 是正定矩阵。 如果把等式的符号改为 >=0，则称 A 是半正定矩阵。

理解：Ax 是对 x 做了线性变换，得到了 y，$x^ty$ 也就是 x 和 y 的点积，大于 0，那也就是俩向量方向一致==。因此，正定、半正定矩阵表示的是一个向量经过该矩阵对应的线性变化后的向量，与其本身的夹角小于 (等于)90 度。==

## 正交矩阵

如果 $$AA^T=A^TA$$

那么 A 就是正交矩阵，正交矩阵的行列都是标准正交向量

如果向量空间的一组基互相正交且长度都是 1，那么怎样的基焦作标准正交基

## 正交投影

正交投影就是吧空间中的某个物体投射到某个平面，有正交投影和透视投影。

**所谓正交投影变换，就是已知盒状可视空间内任意点坐标 (x,y,z)，求解垂直投影到 xy 平面的对应点坐标。**。

# 叉积

两个二维向量的叉积的大小，等于这两个向量构成的平行四边形的面积。同时，这个面积也是有正负号的，如果向量 v 叉积 w，v 在 w 的左侧，则面积为负。

计算叉积就要用到行列式，两个三维向量的叉积结果是一个新的三维向量，这个向量必然与原来两个向量确定的平面垂直，并且其长度与这两个向量张成的平行四边形的面积相同。

# 特征向量、特征值、特征分解与奇异值分解

与行列式类似，矩阵的特征值和特征向量提供了一个新的角度来“描述”矩阵的特性，一个矩阵代表的线性变换通常可以由其特征值和特征向量完全描述。

对于矩阵向量乘积，有两种情况：大多数情况下,向量经过线性变换后离开了其所张成的空间（该向量所在直线所有向量的集合），但是一些特殊的向量留在他们张成的空间中。意味着 **矩阵对它的作用仅仅是拉伸或者压缩** ，矩阵对于该向量的乘法作用只相当于一个标量。也就是 $A\vec{x}=\lambda\vec{v}$,

以下图中的变换矩阵为例，基向量 i 在该矩阵的变换作用后，仅仅是沿着 x 轴方向拉伸了 3 倍，而 i 张成的空间是 x 轴，因此向量 i 经过线性变换后仍然留在其张成的空间中。这些特殊的向量就叫做特征向量，伸缩变换的比例叫做特征值。对于一个矩阵来说，相同特征值的特征向量的集合称之为 **特征空间** 。

![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20250730145326412.png)

二维线性变换矩阵不一定有特征向量，例如旋转 90 度就没有（严格的说是没有实数特征向量），因为每个向量都发生了旋转离开了它张成的空间。

另外，关于特征值，还有几个有趣的定理：

- 矩阵的行列式等于其特征值的乘积
- 矩阵的迹（对角线元素之和）等于其特征值之和
