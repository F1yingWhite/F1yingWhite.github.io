---
title: 线性代数
description: ""
image: ""
published: 2025-07-30
tags:
  - 数学
category: 数学
draft: false
---

# 运算封闭

对于一个集合，如果任意两数在一起运算后结果仍在这个集合，那么这个集合对于这种运算是封闭的。当我定义了一种运算后，所可能产生的所有结果是什么？对于向量而言，问题就是，当我初始拥有了一定数量的向量后，对他（们）进行加法和数乘运算，所可能产生的整个向量集合是什么？这就引出了向量空间的概念

# 线性空间

一组基座标定义了一个坐标系，使用不同的基向量就构造了不同的坐标系，将向量缩放在想加就是线性组合。

俩向量 v,w 所有的线性组合就构成了 v,w 张成的空间。

如果我们定义几个向量是线性相关的，意思就是向量组中有至少一个向量可以用其他的向量的线性表示来组合，也就是他对这个向量组没有贡献，把他从向量组中拿掉不影响组成的空间。相反线性无关就是每一个向量都贡献了一个维度，都缺一不可。

# 基

如果向量空间中的一组向量满足：互相线性无关，张成 V，那么他们就是向量空间 V 的一组基。改空间的任意向量都能表达为基向量的线性组合，基向量的向量数叫做维数，记住 dim(V)，同一个向量空间可以有多组基，他们必定维度相等。

# 矩阵与线性变换

变换本质就是函数，在线性代数中我们输入一个向量，输出另一个向量。

## 线性变换

如果变换前后所有的直线仍是直线且远点保持不变，那么就是线性变换。放在二维直角坐标系这一特定场景下，具体的体现就是施加线性变换后，整个坐标系的原点不变，并使网络线保持平行且等距分布。线性变换后的 v 仍然是变换后的 i 和 j 的线性组合。

![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20250730132830705.png)

实际上我们只要知道基向量的位置是怎么变的，其他的向量线性变换后的位置可以快速得到，

简而言之，选定基之后， **向量刻画对象，矩阵刻画对象的运动，用矩阵与向量的乘法施加运动；矩阵的本质是运动的描述。**

## 矩阵乘法

矩阵表示了一种线性变换。有些时候我们会进行多次线性变换，比如对向量先旋转再剪切，但无论经过多少次，最后的总体作用还是一个线性变换，这样的变换可以看做是由多个独立变换组合成的复合变换。

![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20250730133350573.png)

如上图，我们可以看到矩阵，这里矩阵是从右向左不断叠加的，注意这里的复合矩阵就是现旋转再剪切。如此，把矩阵乘法理解为 **连续的几次线性变换** ，我们也能很容易理解，矩阵 A\*B 的结果和矩阵 B\*A 的结果是不一样的，因为操作顺序的不同，产生的影响也不同。比如，先对 i 和 j 基向量先往 x 轴方向拉伸一倍，再顺时针旋转 90 度，与先旋转 90 度再拉伸，结果肯定不一样。

# 行列式

线性变换中有些吧空间向外拉伸，有的向内积压，测量变换具体对空间产生了多少拉伸和压缩对理解变换很有用。

比如我们的基向量变为了 (3,0) 和（0,2），那么 1,1 面积就扩大了 6 倍。实际上，我们只需要观察这个单位正方形变换后的面积变化比例，就等于知道了其他任意区域的面积变化比例，==因为对于其他任意的方块来说都会有相同的变化，这是由线性变换产生网格线保持平行且等距分布这一特性推断出的。而这个变化的比例，就是我们常说的行列式==。如果说一个线性变换的行列式是 3，那就是说它将一个区域的面积变化为原先的 3 倍。

如果一个线性变换的行列式为 0，则说明它将原来的二维平面压缩到了一条线（甚至一个点）上，此时所有区域的面积都为 0。==换句话说，探究一个矩阵的行列式是否为 0，就能了解这个矩阵对应的线性变换是否将空间压缩到了更低维度==（例如从二维降维到一维空间）。

对于负数的行列式就是换了个向

## 行列式的计算

![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20250730135106561.png)

二维行列式的计算简单，上图可以看出空间面积的变化。

det(M1M2)=det(M1)det(M2) 左边的等式代表先进行 M2 矩阵代表的线性变换再执行 M1 所代表的线性变换之后，面积或者体积所变化的比例。右边的式子是两个线性变换使面积或体积变化的比例的乘积。因为两边线性变换之后的结果是一样的（执行顺序一样），所以比例肯定也是一样的

# 逆矩阵、逆矩阵、秩、列空间、与零空间

## 逆矩阵

矩阵的一大作用是解方程，这里的矩阵表示了某个线性变换，求解方程也就等价于寻找一个向量，让他在经过 A 变换后和向量 V 重合

![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20250730140439034.png)

这个方程组 Ax = v 的解依赖于矩阵 A 所代表的变换，是将原始空间挤压到一个更低维空间，还是保持不变，即 A 的行列式是否是 0.如果行列式不为 0，那么空间维度不变，也就是只有一个向量和 v 重合，

对于矩阵求解，我们计算 A 矩阵的逆矩阵就好了

$$
A^{-1}A\vec{x}=A^{-1}\vec{v}
$$

存在逆矩阵的矩阵叫非奇异矩阵，不存在逆矩阵的矩阵叫奇异矩阵。如果行列式为 0，那么 A 代表变换吧空间压缩到了更低的维度，此时 A 没有逆变换，也就是不能吧低维空间转换到高维。如果想要精确的描述变换后的维度，就引入了心的概念，秩

## 秩

**秩** 就代表了变换后空间的维数。如果变换后所有的向量都落在一条直线上，那么这个变换的秩为 1；如果变换后所有的向量都落在一个二维空间上，那么这个变换的秩为 2。
一个矩阵 A 的列秩是 A 的线性无关的列的极大数目。类似地，行秩是 A 的线性无关的行的极大数目。==矩阵的列秩和行秩总是相等的==，因此它们可以简单地称作矩阵 A 的秩。

## 非方阵

之前我们讨论的矩阵都是方阵，例如用 2\*2 的矩阵表示二维向量到二维向量的变换。那么如何理解非方阵呢？很简单，仍然是线性变换，但是是从某个维度转换为另一个维度的坐标。

以一个 3\*2 的矩阵为例，它的几何意义是将输入的二维空间映射到三围空间上。 矩阵有 2 列表示输入空间有 2 个基向量（因此是二维输入空间），有 3 行表示每一个基向量在变换后用 3 个独立的坐标来描述。

# 点积与正交

从几何角度理解，向量的点积就是一个向量在另一个向量上的投影长度，乘另一个向量的长度，因此点积的主要用途是判断两个向量是否正交。当两向量正交的时候，点积的结果为 0。如果方向相反，点积的结果为负值。当方向完全一致时，点积的值最大。那么这二者是如何联系起来的？

首先我们有一个二维空间到数轴的线性变换，因为这个变换是线性的，因此肯定能有一个一行二列的矩阵 (ux,uy) 来表示（从 2 维压缩到 1 维），这个矩阵也就是变换后的基向量 i,j 的坐标，也就是基向量在新数轴上的投影。

![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20250730142920778.png)

而又因为 1\*2 矩阵与一个二维向量相乘的过程，和将这个矩阵转置过来，与向量做点积的过程相同，所以这个投影的变换必定与某个二维向量有关。由此我们得到结论，任何时候一个线性变换，如果输出空间是一维的数轴，不管这个变换是如何定义的，空间都会存在一个唯一的向量与这个变换相关，施加该线性变换和做点积效果是一样的。换句话说，两个向量的点积，就是将其中一个向量转换为线性变换，施加在另一个向量上。

这是数学中对偶性的一个体现。一个向量的对偶，是它定义的线性变换。一个多维空间到一维空间的线性变换的对偶，就是多维空间中的某个特定向量。

## 正定矩阵

正定矩阵在机器学习特别在矩阵分解中有重要的作用，他和向量点积密切相关

定义:A 是 n 阶对称方阵，如果对任何非零向量 x，都有 $x^TAx>0$ ，就称 A 是正定矩阵。 如果把等式的符号改为 >=0，则称 A 是半正定矩阵。

理解：Ax 是对 x 做了线性变换，得到了 y，$x^ty$ 也就是 x 和 y 的点积，大于 0，那也就是俩向量方向一致==。因此，正定、半正定矩阵表示的是一个向量经过该矩阵对应的线性变化后的向量，与其本身的夹角小于 (等于)90 度。==

## 正交矩阵

如果 $$AA^T=A^TA$$

那么 A 就是正交矩阵，正交矩阵的行列都是标准正交向量（也就是相同的列相乘为 1，不同则为 0（正交））

如果向量空间的一组基互相正交且长度都是 1，那么怎样的基焦作标准正交基

## 正交投影

正交投影就是吧空间中的某个物体投射到某个平面，有正交投影和透视投影。

**所谓正交投影变换，就是已知盒状可视空间内任意点坐标 (x,y,z)，求解垂直投影到 xy 平面的对应点坐标。**。

# 叉积

两个二维向量的叉积的大小，等于这两个向量构成的平行四边形的面积。同时，这个面积也是有正负号的，如果向量 v 叉积 w，v 在 w 的左侧，则面积为负。叉积的结果是一个向量而不是一个标量

计算叉积就要用到行列式，两个三维向量的叉积结果是一个新的三维向量，这个向量必然与原来两个向量确定的平面垂直，并且其长度与这两个向量张成的平行四边形的面积相同。

对于两个三维向量：

$$
\mathbf{a} = \begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix}, \quad
\mathbf{b} = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix}
$$

它们的叉积定义为：

$$
\mathbf{a} \times \mathbf{b} = 
\begin{bmatrix}
a_2b_3 - a_3b_2 \\
a_3b_1 - a_1b_3 \\
a_1b_2 - a_2b_1
\end{bmatrix}
$$

或者用行列式形式表示：

$$
\mathbf{a} \times \mathbf{b} = 
\begin{vmatrix}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
a_1 & a_2 & a_3 \\
b_1 & b_2 & b_3 \\
\end{vmatrix}
$$

对于上面这个行列式，展开之后的结果和定义一致。其实的值相当于 $ab\sin \theta$

# 基变换

我们用坐标来表示向量，首先就要选取坐标系。坐标系就是基向量的体现，如何对俩组基向量进行转换就是基变换。

![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20250731102644844.png)

这里的绿色向量为 i,j，红色向量为 $u,v$,如果选择 i,j 为基向量，那么 u=(3,1),v=（-2,1），那么红色基向量下的坐标可以如下转换为绿色基向量的坐标：$2\begin{bmatrix}3\\1\end{bmatrix}+1\begin{bmatrix}-2\\1\end{bmatrix}=\begin{bmatrix}3\:-2\\1\:1\end{bmatrix}\begin{bmatrix}2\\1\end{bmatrix}=\begin{bmatrix}4\\3\end{bmatrix}\text{。(}P=\begin{bmatrix}3\:-2\\1\:1\end{bmatrix}$),同样的，如果我们以 u,v 为基向量，那么可以得到变换矩阵 Q,其实 PQ 是互逆的。

## 不同基下的空间变换

比如我们逆时针旋转 90°，很容易看出在 i,j 下，变换矩阵为 $A=\left[\begin{matrix}{0}&{-1}\\{1}&0\\\end{matrix}\right]$，那么在红色基向量下如何表示相同的变换 M？

对于红色的一个向量 x,我们可以先把它转换到标准正交基表示，然后在应用 A 变换，然后这用红色向量表示（$P^{-1}$) 就可得到一样的效果了 $$Mx=P^{-1}APx\:\Rightarrow\:M=P^{-1}AP$$

# 特征向量、特征值、特征分解与奇异值分解

与行列式类似，矩阵的特征值和特征向量提供了一个新的角度来“描述”矩阵的特性，一个矩阵代表的线性变换通常可以由其特征值和特征向量完全描述。

对于矩阵向量乘积，有两种情况：大多数情况下,向量经过线性变换后离开了其所张成的空间（该向量所在直线所有向量的集合），但是一些特殊的向量留在他们张成的空间中。意味着 **矩阵对它的作用仅仅是拉伸或者压缩** ，矩阵对于该向量的乘法作用只相当于一个标量。也就是 $A\vec{v}=\lambda\vec{v}$,想要得到特征值，只需要计算 $\det(A-\lambda I)=0$ 就行

以下图中的变换矩阵为例，基向量 i 在该矩阵的变换作用后，仅仅是沿着 x 轴方向拉伸了 3 倍，而 i 张成的空间是 x 轴，因此向量 i 经过线性变换后仍然留在其张成的空间中。这些特殊的向量就叫做特征向量，伸缩变换的比例叫做特征值。对于一个矩阵来说，相同特征值的特征向量的集合称之为 **特征空间** 。

![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20250730145326412.png)

二维线性变换矩阵不一定有特征向量，例如旋转 90 度就没有（严格的说是没有实数特征向量），因为每个向量都发生了旋转离开了它张成的空间。

另外，关于特征值，还有几个有趣的定理：

- 矩阵的行列式等于其特征值的乘积
- 矩阵的迹（对角线元素之和）等于其特征值之和

## 矩阵对角化与特征分解

对角矩阵就是除了对角线外所有元素都是 0 的矩阵，计算行列式，幂运算和逆都很快，因此，如果能把一个矩阵计算到对角矩阵，会极大的方便计算。

在知晓了特征向量和特征值的概念之后，如果转换矩阵的基向量恰好是特征向量的话，则将坐标系变换后（基向量变换后），对应的变换矩阵将变成一个对角矩阵，对角元是特征向量对应的特征值。

![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20250731105525904.png)

比如这里 $\begin{bmatrix}3&1\\0&2\end{bmatrix}$ 的俩特征向量是 (1.,0) 和 (-1,1) 那么原始矩阵 M 经过 $D=A^{-1}MA$ 就可以得到对角矩阵了。而且对角线上的就是特征值

## 奇异值分解

https://www.zhihu.com/question/22237507#answer-16927902

奇异值分解写为 SVD，是线性代数中最基本最核心的理论，因为他能作用于任何矩阵，在 PCA，图像压缩，推荐中有广泛应用。

图片实际上对应一个句子，矩阵上的数值对应像素值，我们记这个矩阵为 A。现在我们对矩阵 A 进行奇异值分解，直观上，奇异值分解进矩阵分解为若干个秩一矩阵之和 $(1)\quad A=\sigma_1u_1v_1^\mathrm{T}+\sigma_2u_2v_2^\mathrm{T}+\ldots+\sigma_ru_rv_r^\mathrm{T}$,这里的 $\sigma$ 就是奇异值，uv 分别表示列向量，注意到每一项 $uv^T$ 都是秩 1 的矩阵，我们假定 $\sigma_{1}\geq\sigma_{2}\dots\geq\sigma_{r}>0$.如果我们只保留大的奇异值而舍弃小的，我们会得到怎么样的矩阵？我们会发现保留的越多，图像越清晰。奇异值往往对应着矩阵的重要信息，而且重要性和奇异值大小正相关。每个矩阵 A 都可以表示为一系列秩为 1 的小矩阵之和。

对于对角矩阵，我们可以看作是水平或垂直方向的拉伸，对于对称矩阵，我们可以找到一组网格线，在上面表示为拉伸（也就是特征值），对于更一般的非对称矩阵，我们无法一定找到网格（因为不一定可对角化）我们退求其次，找到一组网格吗让他在上面可以拉伸和旋转，但是保证变化后的网格互相垂直，这是可以做到的

<img src="https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20250731111149347.png" width="30%" />

对于奇异值分解，对于任意一个矩阵，我们要找到一组两两正交的单位向量，让矩阵作用在上面后得到的新向量两两正交。奇异值就是这组变换后新向量的长度

当矩阵 $M$ 作用在正交单位向量 $v_1$ 和 $v_2$ 上之后，得到 $Mv_1$ 和 $Mv_2$ 也是正交的。令 $u_1$ 和 $u_2$ 分别是 $Mv_1$ 和 $Mv_2$ 方向上的单位向量，即 $Mv_1 = \sigma_1 u_1$，$Mv_2 = \sigma_2 u_2$，写在一起就是

$$
M [v_1 \quad v_2] = [\sigma_1 u_1 \quad \sigma_2 u_2],
$$

整理得：

$$
M = M [v_1 \quad v_2] \begin{bmatrix} v_1^\mathrm{T} \\ v_2^\mathrm{T} \end{bmatrix} = [u_1 \quad u_2] \begin{bmatrix} \sigma_1 & 0 \\ 0 & \sigma_2 \end{bmatrix} \begin{bmatrix} v_1^\mathrm{T} \\ v_2^\mathrm{T} \end{bmatrix}.
$$

这样就得到矩阵 $M$ 的奇异值分解。奇异值 $\sigma_1$ 和 $\sigma_2$ 分别是 $Mv_1$ 和 $Mv_2$ 的长度。很容易可以把结论推广到一般 $n$ 维情形。

下面给出一个更简洁更直观的奇异值的几何意义。先来一段线性代数的推导，不想看也可以略过，直接看**黑体字几何意义部分**：

假设矩阵 $A$ 的奇异值分解为

$$
A = [u_1 \quad u_2] \begin{bmatrix} 3 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} v_1^\mathrm{T} \\ v_2^\mathrm{T} \end{bmatrix},
$$

其中 $u_1$，$u_2$，$v_1$，$v_2$ 是二维平面的向量。根据奇异值分解的性质，$u_1$，$u_2$ 线性无关，$v_1$，$v_2$ 线性无关。那么对二维平面上任意的向量 $x$，都可以表示为：$x = \xi_1 v_1 + \xi_2 v_2$。

当 $A$ 作用在 $x$ 上时，

$$
y = Ax = A [v_1 \quad v_2] \begin{bmatrix} \xi_1 \\ \xi_2 \end{bmatrix} = [u_1 \quad u_2] \begin{bmatrix} 3 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} v_1^\mathrm{T} \\ v_2^\mathrm{T} \end{bmatrix} [v_1 \quad v_2] \begin{bmatrix} \xi_1 \\ \xi_2 \end{bmatrix} = 3\xi_1 u_1 + \xi_2 u_2.
$$

令 $\eta_1 = 3\xi_1$，$\eta_2 = \xi_2$，我们可以得出结论：如果 $x$ 是在单位圆 $\xi_1^2 + \xi_2^2 = 1$ 上，那么 $y$ 正好在椭圆 $\eta_1^2 / 3^2 + \eta_2^2 / 1^2 = 1$ 上。这表明：

> **矩阵 $A$ 将二维平面中单位圆变换为椭圆，而两个奇异值正好是椭圆的两个半轴长，长轴所在的直线是 $\mathrm{span}\{u_1\}$，短轴所在的直线是 $\mathrm{span}\{u_2\}$**。

推广到一般情形：一般矩阵 $A$ 将单位球 $\|x\|_2 = 1$ 变换为超椭球面

$$
E_m = \{y \in \mathbf{C}^m : y = Ax, \, x \in \mathbf{C}^n, \, \|x\|_2 = 1\},
$$

那么矩阵 $A$ 的每个奇异值恰好就是超椭球的每条半轴长度。

![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20250731112355201.png)

对于任意一个 $m \times n$ 的实矩阵 $A$，它的**奇异值分解**可以写成：

$$
A = U \Sigma V^T
$$

其中：

- $U$ 是一个 $m \times m$ 的**正交矩阵**，列向量称为 **左奇异向量**（Left Singular Vectors）
- $\Sigma$ 是一个 $m\times n$ 的**对角矩阵**，对角线上的非负元素称为 **奇异值**（Singular Values），通常记为 $\sigma_1 \geq \sigma_2 \geq \cdots \geq 0$
- $V$ 是一个 $n \times n$ 的**正交矩阵**，列向量称为 **右奇异向量**（Right Singular Vectors）

### 🌟 核心等式推导

从 $A = U \Sigma V^T$ 出发，两边左乘 $A^T$：

$$
A^T A = (U \Sigma V^T)^T (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T = V (\Sigma^T \Sigma) V^T
$$

因为 $U^T U = I$，所以：

$$
A^T A = V D V^T
$$

其中 $D = \Sigma^T \Sigma$ 是一个 $ n\times n$ 对角矩阵，对角线是 $\sigma_i^2$

👉 所以：**$A^T A$ 的特征值是 $\sigma_i^2$，特征向量是 $\mathbf{v}_i$**

同理可得：$A A^T = U (\Sigma \Sigma^T) U^T$，所以 $A A^T$ 的特征向量是左奇异向量 $\mathbf{u}_i$
