---
title: 变分推断
description: ""
image: ""
published: 2025-08-26 10:27:34
tags:
  - 数学
category: 数学
draft: true
updated: 2025-08-26 18:18:12
---

https://www.zywvvd.com/notes/study/probability/elbo/elbo/

在变分贝叶斯方法中，evidence lower bound（ELBO，也叫变分下界）是一些观测数据对数可能性的有用下界。

# 问题定义

- 给定随机观测数据 X，目标是找到真实分布 $p^*$
- 我们可以从 X 中采用 x 来统计分布，但是这样效率低下且难以获得 $p^*$，因此我们需要寻找 $p^*$ 的近似分布。
- 我们定义一组足够大的带参数的分布组 $p_{\theta}$，定义俩个分布之间的损失函数 L，那么我们就可以通过最小化 L 来得到 $\theta$,也就是 $min_{\theta}:(p_{\theta},p^*)$

## 生成模型

由于显示的分布难以描述复杂的 X 分布，我们引入带有隐变量的概率分布：

- 定义隐变量 z，其分布 p(z) 我们可以任意给定（一般是标准高斯分布）
- 定义复杂函数 $f_{\theta}$,操作层面上可以理解为参数为 $\theta$ 的神经网络
- 定义由 p(z) 采样经过 $f_{\theta}$ 生成 X 分布的方法
- 这样我们定义了 (x,z) 的联合分布，因此我们可以采样 z~p,计算 $f_{\theta}(z)$，采样 $x\sim p_{\theta}(\cdot|z)$
也就是我们得到了观测变量和隐变量的生成模型

## 评估模型

现在我们把 $p_{\theta}$ 近似评估模型 $p_{\theta}(x) \approx p^*(x)$,由于等式右侧和 $Z$ 无关，等式左边也需要和 $Z$ 无关的 $X$ 的边缘分布

直接的想法是对 $z$ 积分：

$$
p_\theta(x) = \int p_\theta(x \mid z) p(z) \, dz,
$$可以用蒙特卡洛积分计算，但如果要求精度会很慢，因此我们转向贝叶斯的思路。
根据贝叶斯公式，有：
$$

p_\theta(x) = \frac{p_\theta(x \mid z) p(z)}{p_\theta(z \mid x)}

$$
此时我们有 $p(z)$（我们自己定义的），$p_\theta(x \mid z)$ 我们自己定义的，这二者乘积为联合分布，但是这个联合分布带着 $z$，需要除以后验概率 $p_\theta(z \mid x)$ 才能消除 $z$。
# 解决方案
## 优化近似分布
在参数z上指定一系列分布Q，Q是在这个空间中的一个概率分布，我们需要找到Q中的一个最优概率分布$q^*(z)$,其是和p(z|x)最近的分布。
$$

q^*(z) = \arg\min_{q(z) \in \mathcal{Q}} L(q(z), p(z \mid x))

$$
## ELBO
当我们取 L 为 KL 散度，这个问题就变成了贝叶斯变分问题（VB），我们的目标变为最小化如下KL散度
$$

L(q(z), p(z \mid x)) = \mathrm{KL}(q(z) \parallel p(z \mid x)) \tag{2}

$$

- 我们寻找的最优近似分布为 $q^*(z)$：

$$

q^*(z) = \arg\min_{q(z) \in \mathcal{Q}} \mathrm{KL}(q(z) \parallel p(z \mid x)) \tag{3}

$$

- 展开 KL 项可将 (3) 式转为：

$$

\begin{aligned}

q^*(z) = \arg\min_{q(z) \in \mathcal{Q}} \mathrm{KL}(q(z) \parallel p(z \mid x)) \\

= \arg\min_{q(z) \in \mathcal{Q}} -\int_z q(z) \log \left[ \frac{p(z \mid x)}{q(z)} \right] dz

\end{aligned}

$$

- 继续展开 KL 项：

$$

\begin{aligned}

\mathrm{KL}(q(z) \parallel p(z \mid x))

&= -\int_z q(z) \log \left[ \frac{p(z \mid x)}{q(z)} \right] dz \\

&= \int_z q(z) \log q(z) \, dz - \int_z q(z) \log p(z \mid x) \, dz

\end{aligned}

$$

- 这里关于 $q(z)$ 对 $z$ 积分，其实就是关于 $q(z)$ 的期望，即  
  $$

  \int_z q(z) f(z, \cdot) \, dz = \mathbb{E}_q[f(z, \cdot)]

  $$  
  那么上式能表示成期望形式：

$$

\begin{aligned}

\mathrm{KL}(q(z) \parallel p(z \mid x))

&= \int_z q(z) \log q(z) \, dz - \int_z q(z) \log p(z \mid x) \, dz \\

&= \mathbb{E}_q[\log q(z)] - \mathbb{E}_q[\log p(z \mid x)]

\end{aligned}

$$
- 第二项可以用条件概率公式继续展开：

$$

\begin{aligned}

\mathrm{KL}(q(z) \parallel p(z \mid x))

&= \mathbb{E}_q[\log q(z)] - \mathbb{E}_q[\log p(z \mid x)] \\

&= \mathbb{E}_q[\log q(z)] - \mathbb{E}_q\left[ \log \left( \frac{p(x, z)}{p(x)} \right) \right] \\

&= \mathbb{E}_q[\log q(z)] - \mathbb{E}_q[\log p(x, z)] + \mathbb{E}_q[\log p(x)]

\end{aligned}

$$

- 此时，变成了三项，观察各项，发现第三项里面 $\log p(x)$ 与期望的对象 $q(z)$ 是无关的，所以期望符号可以直接去掉，于是得到：

$$

\begin{aligned}

\mathrm{KL}(q(z) \parallel p(z \mid x))

&= \mathbb{E}_q[\log q(z)] - \mathbb{E}_q[\log p(x, z)] + \mathbb{E}_q[\log p(x)] \\

&= \underbrace{\mathbb{E}_q[\log q(z)] - \mathbb{E}_q[\log p(x, z)]}_{-\mathrm{ELBO}} + \log p(x)

\end{aligned}

\tag{8}

$$

- 此时，我们把前两项称之为 $-\mathrm{ELBO}$（Evidence Lower Bound）。（注意这里是负的 ELBO）

- 那么，关于 $q(z)$ 的 $\mathrm{ELBO}(q)$ 为：

$$

\mathrm{ELBO}(q) = \mathbb{E}_q[\log p(x, z)] - \mathbb{E}_q[\log q(z)] \tag{9}

$$

- 实际计算中，$\mathrm{ELBO}$ 可以表示成以下形式进行计算：

$$

\begin{aligned}

\mathrm{ELBO}(q)

&= \mathbb{E}_q[\log p(x, z)] - \mathbb{E}_q[\log q(z)] \\

&= \mathbb{E}_q[\log p(x \mid z)p(z)] - \mathbb{E}_q[\log q(z)] \\

&= \mathbb{E}_q[\log p(x \mid z)] + \mathbb{E}_q[\log p(z)] - \mathbb{E}_q[\log q(z)] \\

&= \mathbb{E}_q[\log p(x \mid z)] + \mathbb{E}_q\left[ \log \frac{p(z)}{q(z)} \right] \\

&= \mathbb{E}_q[\log p(x \mid z)] - \int_z q(z) \log \frac{q(z)}{p(z)} dz \\

&= \mathbb{E}_q[\log p(x \mid z)] - \mathrm{KL}(q(z) \parallel p(z))

\end{aligned}

$$
回到公式8，我们发现logp(x)是常数，因为他是关于数据集本身的统计，我们成为evidence。我们可以得到
$$

\log p(x)=ELBO(q)+KL(q(z)|p(z|x))

$$

因为左边是常量，我们要做的就是最小化KL散度也就是最大化ELBO(q)
因此，想要找到$q^*(z)$只要最大化q(z)的ELBO就可以了。
