---
title: KL散度
published: 2024-10-08
description: ''
image: ''
tags: [数学]
category: '数学'
draft: false
---

## 定义
小概率事件带来的信息量大，比如我中彩票了，大概率的事件信息量少。因此我们定义信息量为：
$$
I(x)=log_2(\frac{1}{p(x)})=-log_2(p(x))
$$
也就是-log信息发生的概率。

这里取log是因为，如果多个独立事件发生，他们的概率是相乘，信息量就可以相加，可以理解为极大似然

*熵是用来衡量一个分布的信息量的，也是用来计算不确定性*,他的数学定义如下,对于连续的，就把求和改为积分：
$$
H(p)=\sum{p_iI_i}=-\sum{p_i}log(p_i)
$$
均值的熵最大了，相当于什么都没说。如果概率更加聚拢，那么熵越小