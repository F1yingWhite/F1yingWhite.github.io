---
title: KL散度
published: 2024-10-08
description: ssssss
image: ""
tags:
  - 数学
category: 数学
draft: false
last modified: 2024-10-21 12:23
---

# KL 散度简介

KL 散度的概念来自于概率论和信息论中，KL 散度又叫相对熵，互熵等。用来衡量两个概率分布的区别

KL 散度的概念是建立在熵（Entropy）的基础上的，以离散随机变量为例，给出熵的定义:

如果一个随机变量 x 的取值可能是 $x={x_1...x_n}$,对应的概率为 $p_i=p(X=x_i)$,那么随机变量 x 的熵定义为：

$$
H(X) = -\sum^n_{i=1}{p(x_i)\log{p(x_i)}}
$$

> 规定如果 p(x_i)=0,那么 $p(x_i)\log{p(x_i)}=0$

如果有两个随机变量 P，Q,且概率为 p(x),q(x),则 p 相当 q 的相对熵为 (**也就是用 q 来近似 p**)：

$$
D_{KL}(p||q)=\sum^n_{i=1}{p(x)log\frac{p(x)}{q(x)}}
$$

之所以叫相对熵，是因为可以通过两随机变量的交叉熵和信息熵推导到

如果从定义出发，交叉熵就是 $H(p,q)-H(p)=\sum^n_{i=1}{p(x)log\frac{p(x)}{q(x)}}$

正向和反向 KL 散度的区别在于它们关注的重点不同，具体如下：

## 1. **正向 KL 散度** (\( D_{KL}(P || Q) \))

正向 KL 散度衡量的是在用分布 \( Q \) 去近似真实分布 \( P \) 时的损失。这种情况下，它更强调 **覆盖** \( P \) 的所有可能性，尤其是\( P \) 中有概率质量的区域。

- **直观理解**：
  假设 \( P \) 是我们真实的分布，而 \( Q \) 是模型的预测分布。正向 KL 散度更在意模型 \( Q \) 是否覆盖了真实分布 \( P \) 的所有区域。如果 \( Q \) 对某些重要的 \( P \) 的区域分配了较低的概率，KL 散度会惩罚得很严重。这种情况在分类任务中比较适用，因为在有限的输出类别中，模型需要很好地覆盖每一个类别。

- **问题**：
  对于生成任务，正向 KL 可能会导致模型去尝试覆盖所有的模式（包括小概率的区域），这会导致模型在空白区域赋予不合理的概率，生成出不太可能的结果。

## 2. **反向 KL 散度** (\( D_{KL}(Q || P) \))

反向 KL 散度则衡量的是在用真实分布 \( P \) 去近似模型预测分布 \( Q \) 时的损失。它更倾向于让 \( Q \) 专注于预测出较少但更集中的模式，从而避免在那些 \( P \) 中不太可能的区域分配太多概率。

- **直观理解**：
  反向 KL 散度更关注 \( Q \) 生成的样本是否落在 \( P \) 的高概率区域内，它鼓励模型 \( Q \) 生成少量更精确的样本，而不是试图覆盖所有的 \( P \) 的模式。这种方法在生成任务中更为适用，尤其是在输出空间较大且复杂时，它能避免模型生成不合理的结果。

- **问题**：
  反向 KL 可能会忽略 \( P \) 中的一些小概率区域，导致模型不能很好地捕捉到所有的潜在模式，在一些需要全局覆盖的任务中（如分类任务）效果不佳。

## 总结

- **正向 KL 散度** 适用于分类任务，强调模型对所有模式的覆盖，避免遗漏重要的类别。
- **反向 KL 散度** 适用于生成任务，强调模型在主要模式上的精确性，避免生成不合理的样本。
