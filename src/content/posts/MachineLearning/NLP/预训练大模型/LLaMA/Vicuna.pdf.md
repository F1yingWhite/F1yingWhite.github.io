---
title: "[[MachineLearning/NLP/预训练大模型/LLaMA/Vicuna.pdf|Vicuna]]"
description: ""
image: ""
tags:
  - 机器学习
  - 论文阅读
category: 论文阅读
draft: false
published: 2024-10-24
---

> [!PDF|yellow] [Page.1](MachineLearning/NLP/预训练大模型/LLaMA/Vicuna.pdf#page=1&selection=29,0,30,34&color=yellow)
>
> > In this paper, we present the first attempt to use GPT-4 to generate instructionfollowing data for LLM finetuning.
>
> 用 GPT4 来微调大模型

> [!PDF|yellow] [Page.1](MachineLearning/NLP/预训练大模型/LLaMA/Vicuna.pdf#page=1&selection=51,63,53,101&color=yellow)
>
> > mong these methods, Self-Instruct tuning (Wang et al., 2022a) is a simple and effective method of aligning LLMs to human intent, by learning from instruction-following data generated by state-of-the-art instruction-tuned teacher LLMs
>
> Self-Instruct tuning 是简单而高效的方法,通过 SOTA 的教师模型生成指定来进行微调

我们使用 ChatGPT 来作为老师进行自指令微调,根据预训练的 LLaMA 7B 得到了两个模型,分别是 LLaMA-GPT4(根据来自 GPT4 生成的 52K 的英文指令数据) 和 LLaMA-GPT4-CN(在 52K 的中文指令数据上)
