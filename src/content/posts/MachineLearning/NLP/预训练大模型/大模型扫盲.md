---
title: 大模型扫盲
description: ""
image: ""
tags:
  - 机器学习
  - LLM
category: LLM
draft: false
published: 2024-10-23
---

# 基础架构

目前大模型没有什么新的技术,还是沿用最基础的 Transformer 架构,具有独特的 attention 机制,相当于给模型加强了理解能力,能对更多的词基于更高的关注度,同时该方法具有更好的并行和扩展性,能够处理更长的序列.

目前的主流架构分为 Encoder-Decoder,Encoder only 和 Decoder Only.

1. Encoder Only: 只包含编码器,主要使用不需要生成序列的任务,只需要对输入进行编码和单项处理的任务,比如文本分类,情感分析,代表作为 BERT 类
2. Encoder-Decoder,通常用于 seq2seq 任务,比如机器翻译,对话生成
3. Decoder-Only,通常用于序列生成,比如文本生成,机器翻译,适用于需要生成序列的任务,可以从输入编码生成的序列,还可以进行无监督学习,模型还可以进行监督微调适应下游任务.

**Multimodal prompt：** 这类 prompt 包含的信息就更丰富，主要是将不同模态的信息（如文本、图像、音频等）融合到一起，形成一种多模态的提示，以帮助模型更好地理解和处理输入数据。比如在问答系统中，可以将问题和相关图像作为多模态输入，以帮助模型更好地理解问题的含义和上下文背景，并生成更加准确和全面的答案。

大模型的输出是逐字逐句的,输出了上一个词之后才会输出下一个词,并且上一个词会被作为输入放在问题的后面,比如我们: 今天天气怎么样,他会根据我的问题先出第一个字,然后拼在我的问题后面,比如以\<start>开头,一直输出到\<end>停止自己的循环
