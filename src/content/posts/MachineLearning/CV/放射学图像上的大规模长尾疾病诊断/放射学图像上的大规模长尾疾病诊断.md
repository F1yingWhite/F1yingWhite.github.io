---
title: Large-scale Long-tailed Disease Diagnosis on Radiology Images
published: 2024-10-07
description: ''
image: ''
tags: [机器学习,论文阅读]
category: '论文阅读'
draft: false
---

## Abstract
开发一个通用的放射诊断系统能够极大的帮助临床诊断。在这片文章中，我们提出了*RadDuag*,一个支持2D和3D输入的基于transformer的融合模块的基础模型来进行全面的诊断。*RadDiag*能直接或者被fine-tune到别的数据集上进行使用,总之，我们表明，在互联网上公开共享的医疗数据是一种巨大而有价值的资源，有可能支持为医疗保健构建一个通才人工智能。

## 模型设计
我们定义了病例级别的多标签分类问题，我们的RadDiag架构主要包含了两个模块，一个visual encoder和一个基于transformer的fusion模块，把所有的信息结合来进行case-level的诊断
### Encoder
我们的encoder主要包含了ResNet和ViT,可以标记为$v_i = \Phi_{\text{visual}}(x_i) \in \mathbb{R}^d$，其中x的形状为CxHxWx(D),D只有在三维的输入的时候才有。
![image-20241007151820321](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/image-20241007151820321.png)
- 对于ResNet网络，3D的输入使用3D ResNet，然后在Depth维度上使用平均池化，对于2D网络，直接用2D Resnet。保证2D和3D网络的特征map是一样的resolution$d_{res}\times{h}\times{w}$。然后在输入一个*shared ResNet*来的到最终的视觉嵌入
- 对于ViT-based，对于3D scans，首先把输入转化为一系列不重叠的3D cubes，然后输入到MLP projection layer中，转换为vector embeddings，对于2D scans，先转为2D patch，然后使用MLP映射为vector embeddings。因为ViT能够处理token序列，我们能直接输入到*shared ViT*中得到最终的visual embedding。我们使用了可学习的位置参数，3D的是两组，2D的是一组。
### Fusion Module
对于case-level级别的诊断，我们使用科学系的module来聚合信息。首先我们设置了一些可学习的modality embeddings,用p1~pm表示，m表示模态的总数。对于来自模态j的embedding，我们加上对应的modality embedding来表示他是哪个模态来的。再输入到fusion module中