---
title: Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction
published: 2024-09-18
description: ''
image: ''
tags: [机器学习,多模态,论文阅读]
category: '论文阅读'
draft: false 
---

## Introduction

虽然组织学提供了关于细胞和其他组织结构的表型信息，其他的模态能够提供关于预后的互补信号，比如，整体转录组学代表了一个组织中平均的基因表达，可以揭示一个更为丰富的细胞类型和细胞状态的展示，对疾病的预后有着很好的作用。通过结合这两种模式，我们可以把来自转录组学的全局信息和来自 WSI 的空间信息结合。目前大多数的方法使用晚期融合机制，我们设计了一种早期融合的方法，可以明确的**建模局部形态模式和转录组之间的细粒度交叉模态关系。** 与传统的 vision-language 方法相比转录组和组织学的多模态融合存在两个关键的技术挑战：

1. 虽然图像和文本的模态可以明确的标记为对象区域和词语标记，但是语义上对转录组学进行 tokenization 是很有挑战的。由于转录组学数据已经自然地表示为特征向量，许多以前的研究忽略了标记化，直接将整个特征与其他模态拼接，这限制了多模态学习到后期融合操作。也有其他的方法比如把基因划分为不同的功能集表示不同的基因类型以用作 token，但是这些集合指标是了细胞内部相互作用的基本和不完整的描述。因此他们与细粒度的形态缺少语义上的对应关系。相反，我们根据已知的生物途径(biological pathways)对基因组进行标记。Pathway 是已知相互作用的基因集，与特定的细胞功能相关
2. 捕获密集的多模态交互：histology 和 pathology 的 token 可以通过使用自注意力机制的 Transformer 通过捕捉所有标记之间的成对相似性来实现早期融合。但是处理大规模的 histology patch tokens 和 pathway tokens 集合为早期融合带来了巨大的困难，因为 transformer 有二次复杂度,因此该文章开发了一种 patch-pathway 的三种交互方式的新的高效注意力机制。

总结来说，我们的贡献如下：

1. 提出了一种利用已有的知识把基因组转化为 token 的方式
2. 提出了一种内存高效、分辨率无关的多模态 Transformer 模型，用于整合转录组和补丁标记以预测患者生存
3. 一种多层次的可解释框架,可以提供单模态或者多模态的见解

## Related work

使用病理进行生存预测的有很多，大多数贡献都致力于使用 MIL 建模肿瘤的异质性和肿瘤微环境。为此，人们提出了多种池化策略，比如使用图神经网络，多尺度补丁表示等方法

使用 Transformers 进行多模态融合在分类和生成任务中获得了显著的关注，多模态的 tokens 可以被连接起来并输入到一个常规的 Transformer，层次化的 Transformer，或交叉注意力的 Transformer。随着模态的数量和维度增加，典型的序列长度可能会变得过于庞大而无法输入到 vanilla Transformers(也就是传统的)中，因此需要复杂度较低的方法。若干模型提出了自我注意力机制的重新定义以减少内存和计算需求，例如，通过低秩分解来近似自我注意力，使用潜在瓶颈蒸馏，优化 GPU 读写操作或使用稀疏注意力模式。最近，可解释的多模态模型或事后解释方法也作为一个关键的研究领域出现，尤其是在如医疗保健和人机交互等敏感的人工智能协作决策场景中。

## Method

我们提出了 SurvPath，基于组织学和转录组学的多模态生存预测方法。

### Pathway tokenizer from Transcriptomics

由于细胞层次化和复杂性质，想要从基因组中选择合适的单元具有困难.Pathway 由一组特定的基因或子通路组成，代表了这种分析的一个自然推理单元。

给定一组基因组转录数据，标记为 g 和每个 pathway 的组成，我们的目标是构建 pathway-level 的 token.转录组数据可以被看做是表格数据，可以被 MLP 高效的编码，我们使用多层感知机把基因转换为 pathway-level 的 embedding。反正最后得到的数据格式为$N_p×d$

### 病理的 patch token

首先 20x 放大倍数进行切片，然后使用对比学习预训练的 swin-transformer 进行计算得到 embedding。得到的数据维度为$N_h×d$

### 多模态融合

我们希望能够设计一种早期融合的机制来模拟 pathway 和 patch token 之间的密集多模态交互。我们使用 Transformer attention 来测量和聚类 paire 级别的多模态交互.特定来说，我们通过链接 pathway 和 patch token 得到了一个多模态的序列$(N_H+N_P)×d$，通过使用自注意力机制,我们的计算复杂度达到了$n^2$。

$$
X_{Att} = \sigma\left(\frac{QK^T}{\sqrt{d}}\right)V = \begin{pmatrix}
A_{P \rightarrow P} & A_{P \rightarrow H}\\
A_{H \rightarrow P} & A_{H \rightarrow H}
\end{pmatrix}
\begin{pmatrix}
V_P\\
V_H
\end{pmatrix}
$$

为了加速，我们把多模态 transformer 机制转变为内模态和跨模态项

1. pathway<->pathway 的 attention$A_{p->p}$
2. 跨模态的 pathway<->patch 的 attention$A_{p->H}$和$A_{H->p}$
3. patch->patch $A_{H->H}$

因为 patch 的 token 数目远大于 pathway，也就是$N_H>>N_P$,大多数的内存消耗来源于$A_{H->H}$,为了应对这一情况，我们定义 transformer attention 如下：

$$ \hat{\mathbf{X}}_{Att} = \left[ \begin{array}{c} \hat{\mathbf{X}}^{({\cal P})}_{Att} \\ \hat{\mathbf{X}}^{({\cal H})}_{Att} \end{array} \right] = \sigma \left[\frac{1}{\sqrt{d}} \left( \begin{matrix} Q^{({\cal P})}_{{\cal P}\rightarrow {\cal P}} K^{T({\cal P})}_{{\cal P}\rightarrow {\cal P}} & Q^{({\cal P})}_{{\cal P}\rightarrow {\cal H}} K^{T({\cal P})}_{{\cal P}\rightarrow {\cal H}} \\ Q^{({\cal H})}_{{\cal H}\rightarrow {\cal P}} K^{T({\cal H})}\_{{\cal H}\rightarrow {\cal P}} & -\infty \end{matrix} \right) \right] V $$

上面的公式中，我们提前把 h-h 的位置设置为-∞，这样就可以让 softmax 来忽略他,因此计算量大幅减少，如此就可以进行早期融合。这个公式可以看作是一个多模态序列上的稀疏注意力机制，在 patch token 之间存在稀疏注意力

### 多层级可解释性
