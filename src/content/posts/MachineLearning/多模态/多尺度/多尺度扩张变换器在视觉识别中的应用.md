---
title: DilateFormer:Multi-Scale Dilated Transformer for Visual Recognition
published: 2024-10-09
description: ''
image: ''
tags: [机器学习,多模态,多尺度,论文阅读]
category: '论文阅读'
draft: false
---
## Abstract
原始ViT被鼓励在异质两个图像patch中建立长距离联系,而两一个ViT受到CNN的启发,只在小块的patch中建立联系.前者会导致复杂度二次方增长,而后者受到小感受野的限制.在本文中,我们探索了怎么在复杂度和感受野之间进行一个trade-off的取舍.通过分析全局注意力在不同patch之间的交互,我们在浅层观察到两个关键特性,局部性和稀疏性,表明在vit的浅层的全局注意力中存在一定的冗余.因此,我们提出了Mutil-Scale Dilated Attention来在滑动窗口内建模局部和稀疏的patch相互作用,通过金字塔结构,我们通过把MSDA blocks在底层堆积和全局的多头注意力机制在高层堆积形成了我们的网络.

## Introduction
    在过去的一年间,CNN被证明有广泛的作用,这归功于卷积运算的归纳偏置,局部链接和特征共享.但是,卷积只在局部的像素上进行计算而忽略了全局的建模.被NLP领域的工作启发,Transformer被引入视觉任务中进行long-range dependency的建模能力

使用全局注意力,原始的ViT能够在异质的图像块之间进行建模,但是这样会导致二次放的时间复杂度.为了减少时间复杂度,一些工作使用局部的attention,但是者减少了感受野.

在这个任务中,我们探索了高效的ViT来寻找计算复杂度和感受野之间的平衡.根据我们的观察,在浅层的注意力中,大多数注意力只集中在查询patch周围.这种局部性和稀疏性表明，在主流视觉任务的语义建模中，浅层中的远距离斑块大多是不相关的.因此在全局注意力会造成很大的开销.

基于上述的理论,我们擦用滑动窗口扩张注意力操作,他可以在周围区域的稀疏patch中进行自注意力操作.为了在注意力感受区域中获得更多信息,我们还提出了多尺度扩张注意力(MSDA),同时捕获多个尺度上的注意力.MSDA还为不同的头设置了不同的扩张速率,从而能够进行多尺度的表示学习.

## Method
### Sliding Window Dilated Attention
根据在小型vit中观察到的现象,我们提出了SWDA操作,核心在于在滑动窗口中以查询patch为中心进行稀疏的选择.在这些patch上使用自注意力机制,总的来说可以用如下方程表示
$$
X=SWDA(Q,K,V,r)
$$
对于原始特征图位置(i,j)的查询,SWDA稀疏的在以(i,j)为核心,大小为wxw的范围内选择键值对进行自注意力机制.此外,我们使用了膨胀率r来控制稀疏程度.对于边缘的位置,我们使用0来填充

### Mutil-Scale Dilated Attention
为了利用块层次上自我注意机制在不同尺度上的稀疏性，我们进一步提出了一种多尺度扩展注意（MSDA）块来提取多尺度语义信息.

给定一个特征图,我们首先获得对应的QKV,然后我们把特征图的通道划分为n个不同的头每个头使用不同膨胀速率的多尺度SWDA,表示如下:
$$
h_i=SWDA(Q_i,K_i,V_i,r_i)\\
X=Liner(Concat[h1,...,h2])
$$
这里ri是第i个头的膨胀速率,最后所有的hi被concat起来送到一个线性层来进行特征聚合.

通过对不同的头部设置不同的扩张率，我们的MSDA有效地聚集了参与接受域内不同尺度的语义信息，并有效地减少了自我注意机制的冗余，而无需进行复杂的操作和额外的计算成本

### Overall Architecture

