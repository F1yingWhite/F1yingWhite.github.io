---
title: OFA-达摩院多模态模型
published: 2024-10-14
description: ""
image: https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20241014102351.png
tags:
  - 机器学习
  - 多模态
  - 论文阅读
category: 论文阅读
draft: false
---

## Abstract

通用统一的预训练大模型逐渐变成 AI 研究热潮之一.大规模多模态预训练已经成为未来 AI 的基础设施,AI 模型也变得更加通用统一,通用统一的预训练大模型也已成为当前 AI 研究的一大趋势.

**达摩院重点突破统一范式（模态、任务和架构）的通用多模态预训练框架 M6-OFA，希望降低模型在预训练、适配下游模态与任务、以及推理过程中的难度，以便更加便捷地提供预训练、下游任务微调、模型部署、应用发布的大模型全流程服务。** 并且中了 ICML2022.

多模态统一模型 OFA 的核心思想是将多模态任务表达为序列->序列的形式,结合任务特定的 Instruction 在经典的 Transformer encoder decoder 架构中实现多任务预训练,从而实现:

- 架构统一：使用统一的 transformer encoder decoder 进行预训练和微调，不再需要针对不同任务设计特定的模型层，用户不再为模型设计和代码实现而烦恼。
- 模态统一：将 NLP、CV 和多模态任务统一到同一个框架和训练范式，即使你不是 CV 领域专家，也能轻松接入图像数据，玩转视觉、语言以及多模态 AI 模型。
- 任务统一：将任务统一表达成 Seq2Seq 的形式，预训练和微调均使用生成范式进行训练，模型可以同时学习多任务，让一个模型通过一次预训练即可获得多种能力，包括文本生成、图像生成、跨模态理解等。

## OFA 任务效果

![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20241014103358.png)
![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20241014103424.png)
此外,由于是基于 Instruction 做多任务预训练,模型能够根据任务指令的理解做一些没有学习过得任务,比如针对特定区域的 VQA
![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20241014103631.png)

## OFA 基本原理

通用 AI 需要具备模态,任务和模型大小等多个方面的扩展性,谓词,文章提出任务无关,模态无关,任务足够丰富等几个在算法设计上需要满足的性质,并指出现有模型没有同时满足这些性质的各类原因，包括 Pretrain/Finetune 任务表示不一致、额外的 Finetune 任务相关的结构设计、模态输入对某些任务的依赖。OFA 通过一个简单的任务、模态、结构统一的 seq2seq 框架，在满足以上三个性质的前提下，获得了下游诸多图文跨模态任务的 SOTA 表现。

OFA 的实现原理较为简单,核心架构就是经典的 Transformer Encoder Decoder,为了把预训练和微调都融入这个架构,OFA 把多模态和单模态的任务都表示为 Seq2Seq 的形式且使用上述的模型进行训练,预训练和微调都无序增加模型层数.且 OFA 做了关于不同大小的输入以及对于不同多模态和单模态任务如何统一为序列到序列的形式![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20241014104227.png)

## Abstract

在这项工作中，我们追求一个统一的多模态预训练范式，以打破复杂的任务/模态特定定制的支架,因此我们提出了 OFA,一个支持任务和模态无关的架构.OFA 支持图像生成,视觉定位,图像描述,图像分类等多项任务.这些任务都在一个简单的 seq2seq 架构中.OFA 在预训练和微调阶段都遵循基于指令的学习,不需要额外的任务层来处理下游任务.OFA 还可以处理没见过的任务.

## Introduction

建立一个全能的模型能和人一样处理多种模态的输入是 AI 的研究热潮.关键在于是否有一个统一的架构能够同时处理多种模态或单模态.

transformer 架构网络的发展展示了成为通用计算模型的潜力.深度学习中,训练前微调范式在许多领域取得了进展.在少样本学习中,指令微调也有很好的效果.这些成就让统一大模型的出现成为了可能.

为了更好的对开放问题的泛化能力并且保持不同任务的性能并且简单易用,我们的模型有如下三个特点:**1. Task-Agnostic(任务无关):支持不同的任务,与预训练或者微调无关 2. 模态无关:统一输入输出格式在所有任务中共享 3. 任务全面性:足够多样的任务,并且可以稳健的积累泛化能力.**

但是要在下游任务中保持这些特点是困难的,目前的语言和多模态预训练模型容易出现如下问题:

1. 微调的时候存在额外的学习部件,并且这些结构与任务有关,导致预训练和微调之间存在区别
2. 任务特定的形式,这违反了任务无关性,并且在扩展任务全面性时候负担较重
3. 模态表示和下游任务的纠缠:对与视觉语言任务来说,通常将检测到的对象作为图像输入的一部分,虽然效果好,但是依赖于额外的对象检测器,在开放领域问题中经常失效
   因此我们提出了 OFA,通过一个统一的 seq2seq 通过手工设计的指令来达到任务无关.一个 Transformer 架构被用来作为模态无关的引擎.通过大量的单模态和多模态训练来支持任务全面性

## OFA

我们同意了 IO 架构,任务和模态,总体的框架如图所示![image.png](https://picture-bed-1325530970.cos.ap-nanjing.myqcloud.com/20241014120502.png)

### IO 架构

多模态预训练的最常见做法是使用 image-text 对来对 Transformer 进行预训练.这需要数据预处理或者模态特定的适应器.与复杂的对象特征提取器相比,我们直接使用 ResNet 来把图像卷积为 P 个 Patch.类似 GPT 以及 BART,我们使用 byte-pair encodr 来把 text 序列转换为子词序列然后嵌入为特征,

为了处理不同模态的输入在没有任务特定模式的情况下,把不同模态的数据放到一个统一的空间中是十分必要的.一种可行的方案是吧图像文本和对象离散化然后把他们转换为统一词汇表中的 token.

除了表示图像,表示图像中的对象也很重要,因为有一系列与区域相关的任务.我们把对象也表现为一系列离散的 token.具体而言,我们把每个对象的边框提取出来作为位置 token(x1,y1,x2,y2),至于标签,他们本质上是词语,因此可以用 BPE token 表示.

最后我们使用统一的词汇表来表示所有的语言和视觉 token.

架构:我们使用 Transformer encoder-decoder 架构作为一种所有预训练微调和零样本泛化任务的统一框架.其中 encoder 层包括了 self-attention 和 FNN,decer 包含了 self-attention,FFN 和 cross-attention.为了稳定训练并且加速收敛,我们在自注意力机制中加入了头部缩放,并且在注意力后加入了层归一化.对于位置信息,我们在文本和图像分别使用两个绝对位置嵌入,我们解耦了位置相关性与 token 嵌入与 patch 嵌入.文本使用 1D 位置嵌入,图像使用 2D 位置嵌入.

### Task & Modalities

为了在不同的模态和任务之间实现兼容性,我们设计了一个统一的架构,让模型能够泛化从未见过的任务.因此我们需要在一个统一的框架中表示不同模态相关的可能的下游任务.

为了统一任务和模态,我们设计了一种统一的 seq2seq 学习框架.具体来说，我们在所有任务中共享相同的框架，同时通过手工指定的指令来区分任务具体来说，我们在所有任务中共享相同的框架，同时通过手工指定的指令来区分任务.

对于跨模态表征学习,我们设计了5个任务,包括视觉定位(VG),定位描述(GC),图像文本匹配(ITM),图像描述(IC)和视觉问答(VGA).
- 对于VG任务,模型根据输入的图片xi和指令"该文本描述了什么区域"生成区域token<x1,y1,x2,y2>
- GC是VG的逆任务,根据图像和Instruction<这个区域描述了什么?区域<x1,y1,x2,y2>>生成区域的描述
- 对于ITM,我们使用原始的图像对作为正样本,通过将图像和随机替换的标题来构建负样本,让模型生成是/否来进行匹配.输入指令为:<输入图像和描述:xxx匹配吗>
- ...

对单模态表征学习,我们设计了两个视觉任务和一个语言任务.最近，计算机视觉领域生成式自监督学习的进展表明，掩码图像建模是一个有效的预训练任务。在实践中，我们将图像的中间部分进行掩码处理作为输入。模型学习根据损坏的输入以及指定的指令“图像中间部分是什么？”生成中间部分的稀疏编码。此外，我们还添加了目标检测任务。模型学习根据输入图像和指令文本“图像中有哪些物体？”生成人工标注的物体表示，即物体的位置和标签序列。这两个任务加强了在像素和物体层面的表征学习。对于语言表征学习，遵循我们在纯文本数据上对统一模型进行文本填充预训练.通过这种训练方法,我们得到了一个统一的多模态范式.

我们使用交叉熵训练,但是目前分类存在一些问题,比如模型可能会生成无效标签,我们基于prefix tree克服了这个问题

