---
title: MMCL-Survey
description: 多模态持续学习综述
image: ""
published: 2025-08-20
tags:
  - 论文阅读
  - 持续学习
  - 综述
  - 多模态
category: 论文阅读
draft: false
---

# 挑战

MMCL 虽然和传统的 CL 存在关系，但是 MMCL 的挑战超越了仅仅在多模态数据上堆叠 CL 方法的范畴，这种尝试会导致模型产生次优的性能。在 MMCL 中，除了 CL 自身存在的遗忘挑战外，还存在多模态本身的 4 个挑战

1. 模态不平衡：模态不平衡指的是在多模态系统内不同模态的处理或表示不均匀，在数据层面，不同模态的数据可用性在 CL 过程中可能存在显著差异，极端不平衡的情况包括某些模态的缺失；在参数层面，不同模态特定组件的学习可能以不同的速率收敛，导致所有模态整体学习过程的不平衡。这种情况的发生是因为表现更好的模态在优化过程中可能占据主导地位，而其他模态则优化不足。因此，MMCL 模型可能遭受性能下降，有时甚至可能表现得比其单模态对应模型更差 。
2. 复杂的模态交互。模态交互发生在模型组件中，其中多模态输入信息的表示显式地相互作用，主要体现在模态对齐和模态融合中。在模态对齐中，来自一个样本的不同模态数据特征倾向于发散，这种现象被称为空间紊乱。这种发散会导致性能极大的下降。在模态融合中，传统的多模态融合方法会导致表现变差。中的来说，不同模态的数据异质性可能会导致分布和表示，模态之间的交互也可能导致模型发生遗忘。
3. 高计算成本：MMCL 通常采用了预训练的 LM 来作为基础模型，但是微调这些模型带来了极大的开销。增加模态也不可避免的带来的算力开销。
4. 预训练零样本能力的退化：随着预训练模型的进步，MMCL 方法可以配备这些强大的基础模型。因此，这些预训练的多模态模型通常在未见过的任务上表现出零样本能力 ，这使得 MMCL 方法区别于那些通常从零开始训练的传统单模态 CL 方法。然而，在 MMCL 的持续微调过程中，一些来自预训练基础的初始能力，如执行零样本任务的能力，可能会减弱。这种退化风险可能导致未来任务的严重性能下降，在 MMCL 中被称为负向迁移 。这一现象突显了 MMCL 方法必须在保持预训练能力和适应新任务之间维持微妙平衡的重要性。
我们对 MMCL 的方法大体上分为 4 类：基于正则化，基于架构，基于回放和基于提示词。

# 前言

## MMCL 的分类

### Class-incremental Learning

场景 1（类别增量学习，CIL）。对于 $i \neq j$，数据集 $D_i$ 和 $D_j$ 具有不同的输入分布和数据标签空间，即 $p(\mathcal{X}_i) \neq p(\mathcal{X}_j) \land \mathcal{Y}_i \neq \mathcal{Y}_j$ 。在测试阶段无法获取任务身份信息。模型应能对所有已见过的类别进行分类。模型可能需要在测试时推断任务 ID，以确定测试样本可能属于的类别 。需要注意的是，在传统的 CIL 设置中，各任务的数据标签空间是互不相交的，即 $\forall i \neq j, \mathcal{Y}_i \cap \mathcal{Y}_j = \emptyset$ ；然而，在更广义的 CIL 设置中，数据标签空间可能存在重叠，即 $\exists i \neq j, \mathcal{Y}_i \cap \mathcal{Y}_j \neq \emptyset$ 。

### Domain-incremental Learning

场景 2（领域增量学习，DIL）。对于 $i \neq j$，数据集 $D_i$ 和 $D_j$ 具有不同的输入分布，但具有相同的标签空间，即 $p(\mathcal{X}_i) \neq p(\mathcal{X}_j) \land \mathcal{Y}_i = \mathcal{Y}_j$ 。任务身份信息不需要提供。由于所有任务的标签空间相同，模型无需识别具体任务 。

### Task-incremental Learning

场景 3（任务增量学习，TIL）。对于 $i \neq j$，数据集 $D_i$ 和 $D_j$ 具有不同的输入分布和标签空间，即 $p(\mathcal{X}_i) \neq p(\mathcal{X}_j) \land \mathcal{Y}_i \neq \mathcal{Y}_j$ 。在测试时可获得任务身份信息。模型需要学习多个任务，并且在测试时接收到任务 ID 后，能够知道需要执行哪个任务。

此外，还有两个是 MMCL 独有的场景

### Generative Domain-incremental Learning

场景 4（生成式领域增量学习，GDIL）。对于 $i \neq j$，数据集 $D_i$ 和 $D_j$ 具有不同的输入分布和标签空间，即 $p(\mathcal{X}_i) \neq p(\mathcal{X}_j) \land \mathcal{Y}_i \neq \mathcal{Y}_j$。任务身份信息不需要提供。这是 MMCL 中针对生成式任务的一种新场景，例如生成式视觉问答 。CIL 与 GDIL 的区别在于数据集的标签空间和模型输出形式。在 CIL 中，模型预测结果对应于数据集中具体的标签。然而，在 GDIL 中，模型从一个较大的词汇集合中生成输出。数据集的标签只是该词汇集合的一个子集。我们可以将这个词汇集合视为实际的标签空间。因此，所有数据集共享相同的标签空间，即 $\mathcal{Y}'_i = \mathcal{Y}'_j$。我们将这种场景视为领域增量学习，并将其命名为生成式领域增量学习（GDIL）。

### Modality-dynamic Task-incremental Learning

场景 5（模态动态任务增量学习，MDTIL）。在单模态持续学习中，任务序列天然地是模态静态，因为只涉及一种模态。而在 MMCL 中，一方面，如果任务序列是模态静态的，则其属于上述四种场景之一；另一方面，数据集可能包含不同的模态，即存在 $\exists i, j \in \mathcal{T}, \mathcal{I}_i \neq \mathcal{I}_j$，此时任务序列是模态动态的。对于 $i \neq j$，数据集 $D_i$ 和 $D_j$ 具有不同的输入分布和标签空间，即 $p(\mathcal{X}_i) \neq p(\mathcal{X}_j) \land \mathcal{Y}_i \neq \mathcal{Y}_j$。在测试时可获得任务身份信息。我们称这种场景为模态动态任务增量学习（MDTIL）。

## 测试指标

**评价性能（A)**：$A_{t}=\frac{1}{t}\sum^{t}_{i=1}a_{t,i}$

**遗忘度量（F）**：是历史最高的知识减去当前的知识

$$
f_i^t = \max_{s \in \{1, \cdots, t-1\}} \{a_{s,i} - a_{t,i}\}, \quad \forall i < t. \tag{4}
$$

第 $t$ 个任务的平均遗忘定义为：

$$

F_t = \frac{1}{t - 1} \sum_{i=1}^{t-1} f_i^t. \tag{5}

$$

**后向迁移（BWT）**：差值 $a_{t,i} - a_{i,i}$ 表示任务 $t$ 对先前任务 $i$（其中 $i < t$）的影响。对于第 $t$ 个任务，后向迁移衡量其对所有先前任务性能的平均影响。

$$
\text{BWT}_t = \frac{1}{t - 1} \sum_{i=1}^{t-1} a_{t,i} - a_{i,i}. \tag{6}
$$

**前向迁移（FWT)**: 设 $\bar{b}_i$ 为第 $i$ 个任务在随机初始化下的性能。差值 $a_{i,t} - \bar{b}_t$ 衡量了任务 $i$ 对未来任务 $t$（其中 $i < t$）的影响。对于第 $t$ 个任务，前向迁移定义为：

$$
\text{FWT}_t = \frac{1}{t - 1} \sum_{i=2}^{t} a_{i-1,i} - \bar{b}_i. \tag{7}
$$

**Zero-shot transfer**，用于衡量在训练完前序任务后，模型在第 $t$ 个任务（$t > 1$）上保留的零样本能力水平。对于第 $t$ 个任务：

$$
\text{Transfer}_t = \frac{1}{t - 1} \sum_{i=1}^{t-1} a_{i,t}. \tag{8}
$$

# 方法

## 基于正则化

### 显示正则化

显式正则化方法直接为模型参数分配重要性，并在参数偏离先前找到的解时对其进行不同程度的惩罚，设 $\mathcal{L}_{\text{single},t}$ 为模型在学习第 $t$ 个任务时，单任务设置下的损失。则持续学习的总损失定义为 $\mathcal{L}_t = \mathcal{L}_{\text{single},t} + \lambda_E \mathcal{L}_{E,t}$，其中 $\mathcal{L}_{E,t}$ 是正则化项，超参数 $\lambda_E$ 用于平衡主任务学习与正则化之间的关系。$\mathcal{L}_{E,t}$ 可以表述如下：

$$
\mathcal{L}_{E,t} = \sum_i b_i \left( \theta_i - \theta_{t-1,i}^* \right)^2,
\tag{9}
$$

其中，$\theta_i$ 和 $\theta_{t-1,i}^*$ 分别表示参数向量 $\boldsymbol{\theta}$ 和最优参数 $\boldsymbol{\theta}_t^*$ 的第 $i$ 个元素，$b_i$ 表示对应参数的重要性权重。

### 隐式正则化

和直接存储上一个或者全部任务的最有状态不同，隐式正则化方法通常专注于最小化模型在先前学习任务上的输出，从而降低遗忘风险。与直接惩罚参数变化的显式正则化不同，隐式正则化方法仅在参数变化最终导致模型输出改变时才施加惩罚。通常会基于蒸馏。当模型学习第 $t$ 个任务时，持续学习损失表示为 $\mathcal{L}_t = \mathcal{L}_{\text{single},t} + \lambda_I \mathcal{L}_{I,t}$，其中 $\mathcal{L}_{I,t}$ 是正则化项，超参数 $\lambda_I$ 用于损失平衡。$\mathcal{L}_{I,t}$ 包含 KD，可表述如下

$$
\mathcal{L}_{I,t} = \mathcal{L}_{KD}(y_{t-1}, y_t)
= 
\begin{cases}
-\sum_i y_{t-1,i} \log y_{t,i} & \text{交叉熵损失} \\
\|y_{t-1} - y_t\|_2^2 & \text{L2损失},
\end{cases}
\tag{10}
$$

## 基于架构

基于架构的方法采用直观直接的策略来学习任务，通过使不同的模型参数处理不同的任务。基于正则化的方法共享所有参数来学习任务，这使得它们容易受到任务间干扰的影响 ：当正向知识迁移为负时，记住旧任务会严重干扰新任务的学习，导致性能下降 。相比之下，基于架构的方法通过引入任务特定组件来减少任务间干扰。根据模型设计的不同，基于架构的方法分为两种类型：**固定架构和动态架构**。我们在图中提供了基于固定架构和动态架构方法的代表性架构概述。

### 固定架构

固定架构方法旨在通过为各个任务利用不同部分的参数来减少任务间干扰并缓解遗忘。通常采用硬参数掩码或软参数掩码等技术来在固定架构内实现这种任务特定的参数分配。

### 动态架构

动态架构方法在引入新任务时适应模型结构，通常通过添加新模块来扩展。与在固定模型上运行的方法不同，动态架构方法通常能够随着每个新任务的增加而增加模型容量，从而确保性能最终不受初始容量的限制。值得注意的是，如果模型具有任务特定组件并在测试期间接收任务 ID（TIL 场景），主要目标仍然是避免遗忘；然而，模型还应有效地学习跨任务的共享知识，并在性能与计算复杂度之间取得平衡。

## 基于回放

基于回放的方法使用一个缓冲区 $\mathcal{M}$ 来存放先前的数据从而在学习新任务的同时帮助保持早期知识。这种通过重放实例的方法避免了基于正则化方法的严格约束，并绕过了基于架构方法中动态修改网络结构带来的复杂性。基于回放的方法分为两个子方向：直接回放（direct replay）和伪回放（pseudo replay）。

$$
\mathcal{L}_t = \frac{1}{|D_t \cup \mathcal{M}_t|} \sum_{(x_i, y_i) \in (D_t \cup \mathcal{M}_t)} \ell(f(x_i), y_i). \tag{11}
$$

### 直接回放

直接回放存储了少量的旧数据，关键在于如何选择旧数据。

### 隐式回放

为了避免直接回放方法中的额外存储需求和隐私问题，伪回放最近受到了关注。这种方法涉及使用生成模型来学习先前阶段的数据分布，然后在当前阶段回放生成的数据。

## 基于提示词

随着大模型的快速发展及其在 CL 设置中的应用，基于提示的方法最近应运而生，以更好地利用预训练期间获得的丰富知识。与之前通常需要大量微调或架构修改的方法不同，这些方法具有需要最小模型调整和减少广泛微调需求的优势。基于提示的方法范式涉及通过在连续空间中应用少量提示参数来修改输入，使模型在学习额外任务特定信息的同时保持其原始知识。因此，它们天生能够应对挑战 3：MMCL 设置中的高计算成本，和挑战 4：预训练零样本能力的退化。
